                                                             February 26th, 2024


It's been a while since I wrote a diary, but I am also not a complete stranger
to it.  Like, whenever I was learning something new, I would lead a diary to
see which obstacles I face and how I, hopefully, overcome them.

At this moment, the latest check-in is 10273831c78b1d18a1b87dc7f56b9c6d0d8b7787
it is a couple of check-ins after the first time I got credible results for
Navier-Stokes equatins, which was: bf7b654e0f58f9dc99faa99219be2da9a5622879

The current situation is that the tests from 1 to 5 work well on GPUs, show a
good speed-up, but test 6 is without any GPU functionality.  The reason is not
that wasn't done before, it was, but the implementation was a bit of a mess, as
if I was frentically inserting GPU (OpenACC) commands just to get any speed-up.
There was no real substance, the analithical thinking was absent.

Hence, the goal today, maybe the goal from this point on, would be to go slowly,
one step at a time, browsing profiling data and searching for opportunitues to
speed the code up.

Task 1: Find a case (which in current state of the code development means:
modify the sources in such a way to make the pressure solution the most CPU-
intensive part of the code, because that's how things normally are in CFD.
So not some case which converges after a couple of dozens of time steps and
then uses CPU time to save results or do something silly, something more sub-
stantial, more relevant to a typical CFD simulation.

In order to achieve that, I did the following changes to Test_006.f90:

-  integer, parameter       :: N_STEPS = 600 ! spend enough time on device
-  integer, parameter       :: N_ITERS =   6 ! spend enough time on device
+  integer, parameter       :: N_STEPS =   3 ! spend enough time on device
+  integer, parameter       :: N_ITERS =   3 ! spend enough time on device

Changed the number of time steps to 3 only, with 3 iterations each, but with
the grid size of 200^3.  In other words, changes in test_006_cube.ini read:

-grid_nx   40
-grid_ny   40
-grid_nz   40
+grid_nx  200
+grid_ny  200
+grid_nz  200

Since the program does not have any under-relaxation, time step is limited to
roughly 1 / nx, so in this case, another modification in Test_006.f90 is:

-  dt = 0.025
+  dt = 1.0 / 200.0

I compile the program with:

make GPU=no

This will invoke Nvidia Fortran compiler, and the profiling results read:

 #=================================================================================#
 #                              CPU usage statistics                               #
 #---------------------------------------------------------------------------------#
 #                      Total CPU time: 000:05:59 [hhh:mm:ss]                      #
 #---------------------------------------------+-----------------------------------#
 #        Description of the activity:         |            Spent time:            #
 #---------------------------------------------+-----------------+-----------------#
 # - CG_for_Pressure                           |   236.19 [s]    |     65.64 %     #
 # - Grad_Pressure                             |    78.26 [s]    |     21.75 %     #
 # - Save_Vtk_Vector                           |    13.20 [s]    |      3.67 %     #
 # - CG_for_Momentum                           |     9.34 [s]    |      2.59 %     #
 # - Save_Vtk_Scalar                           |     4.65 [s]    |      1.29 %     #
 # - Test_006                                  |     4.41 [s]    |      1.23 %     #
 # - Insert_Diffusion_Bc                       |     3.50 [s]    |      0.97 %     #
 # - Insert_Volume_Source_For_Pressure         |     3.28 [s]    |      0.91 %     #
 # - Correct_Velocity                          |     3.14 [s]    |      0.87 %     #
 # - Add_Advection_Term                        |     2.42 [s]    |      0.67 %     #
 # - Form_Diffusion_Matrix                     |     0.42 [s]    |      0.12 %     #
 # - Add_Pressure_Term                         |     0.32 [s]    |      0.09 %     #
 # - Add_Inertial_Term                         |     0.32 [s]    |      0.09 %     #
 # - Calculate_Grad_Matrix                     |     0.24 [s]    |      0.07 %     #
 # - Form_Pressure_Matrix                      |     0.15 [s]    |      0.04 %     #
 #---------------------------------------------+-----------------+-----------------#

This looks good, because CG for pressure takes 66% of the time.  That also
completes the Task 1.

Task 2: Decide what you want to port to GPU's (device).  At this point, it is a
no brainer.  One should port the solution of the CG for pressure to device.
In order to achieve that, one should transfer the pressure matrix, pressure
correction variable and also the source to "device" and see how the solution
of pressure correction alone speeds the overal computation up.

But, there is a catch with that.  In the code, OpenACC directives are inserted
in more places than just CG solver for pressure.  Solver for momentum equations
is the same as the one for pressure, so as soon as the program is compiled with
directives for OpenACC, necessary data transfers (to and from the device) will
be needed for velocity components as well.  The same is true for gradients.
The subroutines for calculation of gradients have OpenACC directives, and one
should insure that they work too.

Task 3: Port the solution of Navier-Stokes equations to GPUs.  As described
above, the first port to GPUs will have to be quite a big one, and the
question is how to take it?  Where to perform transfer of data?  All in the
Test_006 (good because all is in place, but the function would grow too much
and it might not be possible to assing all the details of numerical subroutines.
Alternative would be to make all calls for data transfer in the called functions
but that was requiring complex logic which was difficult to follow.

So, I decided for a combined approach:

- In the main function (represented by Test_006) transfer all the data which
will stay monolithic on the device (it won't change) such as linear system
matrices and gradient calculation matrices.  In addition, also reserve memory
on the device for data which will change on the device, such as velocity
components, pressure correction, gradient components, right hand side vector
and vectors which are a part of the CG linear solver (called p, q and r).
To avoid repetition, the reader is referred to Test_006.f90, all the lines
which end with ! <- GPU

- In the functions which perform tasks on GPUs, transfer data to the device
before each call to accelerated function, and fetch what was computed back
to the host.  This approach is rather pragmatic and entails the ackowledgement
that majority of the functions are still performed on the host (CPU).  Such
functions are actually only three in the current implementation:

  Process_Mod/Compute_Momentum and
  Process_Mod/Compute_Pressure
  Field_Mod/Grad_Pressure.f90

In the first two, the righ-hand side estimated on the host (in all other pro-
cedures in Process_Mod except these two) has to be transfered to the device
before a call to CG solver, and the solution from the solver has to be
transferred back to host.  But, one should be careful about the computation
of momentum, because velocity components calculated on the device are not
the same as those on the host after velocity correction.  Therefore, to en-
sure a convergence history on the GPUs as simular as possible on the CPU,
one should also transfer the lates value of corrected velocity components
to the device, since they serve as initial guess to the CG solver.

The third one, Grad_Pressure from Field_Mod also performs calculations on
GPUs, and for it to work properly, one has to update the device with the
latest values of variables, and update host with calculated gradient com-
ponents.

All the changes mentioned above are designated with the symbol ! <- GPU_1
starting around column 65.

Task 4: run the simulations on GPUs.  In this step, I compiled the program
with a simple 'make' command, without any options passed, ran the simulations
and obtained the following results:

 #=================================================================================#
 #                              CPU usage statistics                               #
 #---------------------------------------------------------------------------------#
 #                      Total CPU time: 000:00:53 [hhh:mm:ss]                      #
 #---------------------------------------------+-----------------------------------#
 #        Description of the activity:         |            Spent time:            #
 #---------------------------------------------+-----------------+-----------------#
 # - CG_for_Pressure                           |    13.40 [s]    |     24.93 %     #
 # - Save_Vtk_Vector                           |    12.74 [s]    |     23.71 %     #
 # - Test_006                                  |     5.75 [s]    |     10.69 %     #
 # - Save_Vtk_Scalar                           |     4.59 [s]    |      8.54 %     #
 # - Insert_Diffusion_Bc                       |     3.50 [s]    |      6.51 %     #
 # - Correct_Velocity                          |     3.34 [s]    |      6.21 %     #
 # - Insert_Volume_Source_For_Pressure         |     3.25 [s]    |      6.05 %     #
 # - Add_Advection_Term                        |     2.45 [s]    |      4.55 %     #
 # - Grad_Pressure                             |     1.71 [s]    |      3.19 %     #
 # - Compute_Momentum                          |     0.82 [s]    |      1.53 %     #
 # - CG_for_Momentum                           |     0.52 [s]    |      0.96 %     #
 # - Form_Diffusion_Matrix                     |     0.45 [s]    |      0.83 %     #
 # - Add_Pressure_Term                         |     0.32 [s]    |      0.60 %     #
 # - Add_Inertial_Term                         |     0.32 [s]    |      0.60 %     #
 # - Calculate_Grad_Matrix                     |     0.24 [s]    |      0.44 %     #
 # - Compute_Pressure                          |     0.18 [s]    |      0.34 %     #
 # - Form_Pressure_Matrix                      |     0.17 [s]    |      0.32 %     #
 #---------------------------------------------+-----------------+-----------------#

The profiling data shows that the code runs six time faster on GPUs, and that
linear solver for pressure alone went from 236 to 13 seconds, a very good
improvement.


                                                               February 27, 2024


I am currently on the checkin 81494c2f1d303af524a0ed9b0fa1744139193237
I ran the simulation (without saving the .vtk file because I know that is slow,
and observed the following:

 #=================================================================================#
 #                              CPU usage statistics                               #
 #---------------------------------------------------------------------------------#
 #                      Total CPU time: 000:00:38 [hhh:mm:ss]                      #
 #---------------------------------------------+-----------------------------------#
 #        Description of the activity:         |            Spent time:            #
 #---------------------------------------------+-----------------+-----------------#
 # - CG_for_Pressure                           |    13.41 [s]    |     35.10 %     #
 # - Test_006                                  |     5.04 [s]    |     13.20 %     #
 # - Insert_Diffusion_Bc                       |     3.49 [s]    |      9.14 %     #
 # - Correct_Velocity                          |     3.34 [s]    |      8.75 %     #
 # - Insert_Volume_Source_For_Pressure         |     3.26 [s]    |      8.53 %     #
 # - Update_Host                               |     2.56 [s]    |      6.70 %     #
 # - Add_Advection_Term                        |     2.52 [s]    |      6.59 %     #
 # - Grad_Pressure                             |     1.73 [s]    |      4.52 %     #
 # - Update_Device                             |     0.83 [s]    |      2.17 %     #
 # - CG_for_Momentum                           |     0.52 [s]    |      1.35 %     #
 # - Form_Diffusion_Matrix                     |     0.45 [s]    |      1.18 %     #
 # - Add_Pressure_Term                         |     0.33 [s]    |      0.86 %     #
 # - Add_Inertial_Term                         |     0.32 [s]    |      0.85 %     #
 # - Calculate_Grad_Matrix                     |     0.24 [s]    |      0.63 %     #
 # - Form_Pressure_Matrix                      |     0.17 [s]    |      0.45 %     #
 #---------------------------------------------+-----------------+-----------------#

The Add_Advection_Term takes a relativelly high percentage, 6.59 %, it seemed
a lot for a relativelly simple operation.  As a reminder, the function has a
loop of this form:

  !-------------------------------------------!
  !   Browse through all the interior faces   !
  !   (This creates race conditions on GPU)   !
  !-------------------------------------------!
  do s = Grid % n_bnd_cells + 1, Grid % n_faces
    c1 = Grid % faces_c(1, s)
    c2 = Grid % faces_c(2, s)

    ! Flux goes from c1 to c2 (kg/m^3 * m^3/s * m/s = kg m/s^2 - OK)
    if(v_flux(s) .gt. 0) then
      b(c1) = b(c1) - DENS * v_flux(s) * ui % n(c1)
      b(c2) = b(c2) + DENS * v_flux(s) * ui % n(c1)
    end if

    ! Flux goes from c2 to c1 (kg/m^3 * m^3/s * m/s = kg m/s^2 - OK)
    if(v_flux(s) .lt. 0) then
      b(c1) = b(c1) - DENS * v_flux(s) * ui % n(c2)
      b(c2) = b(c2) + DENS * v_flux(s) * ui % n(c2)
    end if

  end do

which cannot be ported to GPUs due to race conditions appaering when browsing
through faces.  In order to change this loop into something which can be
accelerated on GPUs, is to abandon the face-based data structure, and go for
a cell-based data structure instead.  After all, the sparse matrix vector
products, relying on sparse matrish in CSR format, are in essence already
cell-based data structure; every row know its columns, meaning that every cell
knows its cell neighbors.

Another example is gradient calculation, which is cell-based since some time
now.  As in the case of sparse matrix vector products, in case of gradient
calculation we rely on knowledge of each cell neighbors; albeit in this case
not store in matrix's rows and columns, but in the data structure cells_c
and cells_n_cells.

However, in this particular case, case of updating advection terms with values
in neighboring cells but also the volume flux at faces, cell-cell connectivity
alone is not sufficient, we also need faces enclosing each cell, since they
hold the stored volume fluxed.  In order to implement that, a new data struc-
ture was introduced, called cells_f (storing each cell's faces) which has the
same size as cells_c (one neighbouring cell corresponds to one face they share)
and the number of faces around the cell is the same as number of neighboring
cells (cells_n_cells).  The implementation is rather straightforward, as one
can see in Grid_Mod.f90 and Grid_Mod/Create_Grid, in the lines holding the
symbols: !-> GPU_2.  With the cells_f structure in place, the above face-based
loop can be translated to cell-based loop as following:

  !-------------------------------------------!
  !   Browse through all the interior cells   !
  !-------------------------------------------!
  do c1 = 1, Grid % n_cells
    do i_cel = 1, Grid % cells_n_cells(c1)
      c2 = Grid % cells_c(i_cel, c1)
      s  = Grid % cells_f(i_cel, c1)
      if(c2 .gt. 0) then

        if(c1 < c2) then
          ! Flux goes from c1 to c2 (kg/m^3 * m^3/s * m/s = kg m/s^2 - OK)
          if(v_flux(s) .gt. 0) then
            b(c1) = b(c1) - DENS * v_flux(s) * ui % n(c1)
          end if
          ! Flux goes from c2 to c1 (kg/m^3 * m^3/s * m/s = kg m/s^2 - OK)
          if(v_flux(s) .lt. 0) then
            b(c1) = b(c1) - DENS * v_flux(s) * ui % n(c2)
          end if
        end if

        if(c1 > c2) then
          ! Flux goes from c2 to c1 (kg/m^3 * m^3/s * m/s = kg m/s^2 - OK)
          if(v_flux(s) .gt. 0) then
            b(c1) = b(c1) + DENS * v_flux(s) * ui % n(c2)
          end if
          ! Flux goes from c1 to c2 (kg/m^3 * m^3/s * m/s = kg m/s^2 - OK)
          if(v_flux(s) .lt. 0) then
            b(c1) = b(c1) + DENS * v_flux(s) * ui % n(c1)
          end if
        end if

      end if
    end do

  end do

What is important to note is that the loop updates only values in c1 (other-
wise it would have the same race condition issues as the face-based loop), and
that one should distinguish between two cases: when c1 < c2, whose implemen-
tation is the same as it was for faces before, and c1 > c2, where everything
changes the sign and the order to compared to face-based browsing.

Although this loop worked, it is a bit long and not very elegant to implement
GPU acceleration.  Too many nested ifs, just to mention one issue.  The next
version replaced the checks "if v_flux(s) .gt. 0) with min and max functions
in the following form:

  !-------------------------------------------!
  !   Browse through all the interior cells   !
  !-------------------------------------------!
  do c1 = 1, Grid % n_cells
    do i_cel = 1, Grid % cells_n_cells(c1)
      c2 = Grid % cells_c(i_cel, c1)
      s  = Grid % cells_f(i_cel, c1)
      if(c2 .gt. 0) then

        if(c1 < c2) then
          b(c1) = b(c1) - DENS * v_flux(s) * ui % n(c1) * max(v_flux(s), 0.0)
          b(c1) = b(c1) - DENS * v_flux(s) * ui % n(c2) * min(v_flux(s), 0.0)
        end if

        if(c1 > c2) then
          b(c1) = b(c1) + DENS * v_flux(s) * ui % n(c2) * max(v_flux(s), 0,0)
          b(c1) = b(c1) + DENS * v_flux(s) * ui % n(c1) * min(v_flux(s), 0.0)
        end if

      end if
    end do

  end do

Here, instead of checking if v_flux is greater or smaller than zero, the
max(v_flux(s), 0) and min(v_flux(s), 0) are used insted.  It is more compact,
but proved also to be faster than the previous version.

The next thing which might be done in order to make the code more compact and
more efficient is to replace the "if(c1 < c2)" and "if(c2 > c1)" statements with
merge command, as illustrated here:

  !-------------------------------------------!
  !   Browse through all the interior cells   !
  !-------------------------------------------!

  !$acc parallel loop
  do c1 = 1, n

    b_tmp = b(c1)

    !$acc loop seq
    do i_cel = 1, cells_n_cells(c1)
      c2 = cells_c(i_cel, c1)
      s  = cells_f(i_cel, c1)
      if(c2 .gt. 0) then
        den_v_u1 = DENS * v_flux(s) * ui_n(c1)
        den_v_u2 = DENS * v_flux(s) * ui_n(c2)
        b_tmp = b_tmp - den_v_u1 * max(v_flux(s), 0.0) * merge(1,0, c1.lt.c2)
        b_tmp = b_tmp - den_v_u2 * min(v_flux(s), 0.0) * merge(1,0, c1.lt.c2)
        b_tmp = b_tmp + den_v_u2 * max(v_flux(s), 0.0) * merge(1,0, c1.gt.c2)
        b_tmp = b_tmp + den_v_u1 * min(v_flux(s), 0.0) * merge(1,0, c1.gt.c2)
      end if
    end do
    !$acc end loop

    b(c1) = b_tmp

  end do
  !$acc end parallel

Here, the OpenACC have already been introduced, as well as local variable b_tmp,
which facilitates compiler to understand the loop is safe to parallelize, and
to new variables (den_v_u1 and den_v_u2) are introduced to save on a couple of
multiplications insid the loop.

With these changes, the profiling results read:

 #=================================================================================#
 #                              CPU usage statistics                               #
 #---------------------------------------------------------------------------------#
 #                      Total CPU time: 000:00:37 [hhh:mm:ss]                      #
 #---------------------------------------------+-----------------------------------#
 #        Description of the activity:         |            Spent time:            #
 #---------------------------------------------+-----------------+-----------------#
 # - CG_for_Pressure                           |    13.42 [s]    |     35.66 %     #
 # - Test_006                                  |     5.20 [s]    |     13.81 %     #
 # - Insert_Diffusion_Bc                       |     3.50 [s]    |      9.29 %     #
 # - Correct_Velocity                          |     3.34 [s]    |      8.86 %     #
 # - Insert_Volume_Source_For_Pressure         |     3.30 [s]    |      8.77 %     #
 # - Update_Host                               |     2.83 [s]    |      7.51 %     #
 # - Update_Device                             |     2.19 [s]    |      5.81 %     #
 # - Grad_Pressure                             |     1.74 [s]    |      4.62 %     #
 # - CG_for_Momentum                           |     0.50 [s]    |      1.33 %     #
 # - Form_Diffusion_Matrix                     |     0.45 [s]    |      1.19 %     #
 # - Add_Pressure_Term                         |     0.32 [s]    |      0.86 %     #
 # - Add_Inertial_Term                         |     0.32 [s]    |      0.84 %     #
 # - Calculate_Grad_Matrix                     |     0.24 [s]    |      0.64 %     #
 # - Form_Pressure_Matrix                      |     0.17 [s]    |      0.46 %     #
 # - Add_Advection_Term                        |     0.13 [s]    |      0.35 %     #
 #---------------------------------------------+-----------------+-----------------#

Add_Advection_Term when from using 2.52 [s] to only 0.13 [s], a whopping
speed up of a factor 20!



                                                                   March 1, 2024

At this point I have both periodicity and the possibility to insert obstacles in
the code, although the implementation of the latter went faster than I wanted,
I pushed, by mistake, things before thoroughly checking them, really.  Today, I
will try to fix and / or document a few things.



